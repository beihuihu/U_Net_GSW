{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0-rc3\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import geopandas as gps\n",
    "import rasterio                  # I/O raster data (netcdf, height, geotiff, ...)\n",
    "import rasterio.warp             # Reproject raster samples\n",
    "from rasterio import windows\n",
    "from rasterio import features\n",
    "import fiona                     # I/O vector data (shape, geojson, ...)\n",
    "import pyproj                    # Change coordinate reference system\n",
    "from osgeo import gdal, ogr, osr\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.geometry import mapping, shape\n",
    "import cv2\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import numpy as np               # numerical array manipulation\n",
    "import pandas as pd\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import PIL.ImageDraw\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import sys\n",
    "from core.UNet import UNet\n",
    "from core.losses import tversky, accuracy, dice_coef, dice_loss,IoU, precision, recall\n",
    "from core.optimizers import adaDelta, adagrad, adam, nadam\n",
    "from core.visualize import display_images\n",
    "# from core.dataset_generator import DataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt  # plotting tools\n",
    "%matplotlib inline\n",
    "import warnings                  # ignore annoying warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto(\n",
    "    #device_count={\"CPU\": 64},\n",
    "    allow_soft_placement=True, \n",
    "    log_device_placement=False)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.trained_model_path: G:\\U-Net\\U-Net\\saved_models\\UNet\\lakes_20230622-1743_AdaDelta_tversky_01_512.h5\n"
     ]
    }
   ],
   "source": [
    "# Required configurations (including the input and output paths) are stored in a separate file (such as config/RasterAnalysis.py)\n",
    "# Please provide required info in the file before continuing with this notebook. \n",
    " \n",
    "from config import RasterAnalysis\n",
    "# In case you are using a different folder name such as configLargeCluster, then you should import from the respective folder \n",
    "# Eg. from configLargeCluster import RasterAnalysis\n",
    "config = RasterAnalysis.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model \n",
    "OPTIMIZER = adaDelta\n",
    "# OPTIMIZER = adam\n",
    "# OPTIMIZER=tf.train.experimental.enable_mixed_precision_graph_rewrite(OPTIMIZER)\n",
    "model = load_model(config.trained_model_path, custom_objects={ 'dice_loss':dice_loss, 'accuracy':accuracy, 'IoU': IoU,'precision':precision, 'recall':recall}, compile=False)\n",
    "model.compile(optimizer=OPTIMIZER, loss=tversky, metrics=[dice_coef, dice_loss, accuracy,recall, precision,IoU])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to add results of a patch   to    the total results of a larger area. \n",
    "#The operator could be min (useful if there are too many false positives), max (useful for tackle false negatives)\n",
    "#res:mask [rows,cols] predition=np.squeeze(prediction[i], axis = -1) (col, row, wi, he) = batch_pos[i]\n",
    "def addTOResult(res, prediction, row, col, he, wi,operator,padding):\n",
    "    currValue = res[row:row+he, col:col+wi]\n",
    "    newPredictions = prediction[:he, :wi]\n",
    "# IMPORTANT: MIN can't be used as long as the mask is initialed with 0!!!!! \n",
    "#If you want to use MIN initial the mask with -1 and handle the case of default value(-1) separately.\n",
    "    if operator == 'MIN': # Takes the min of current prediction and new prediction for each pixel  \n",
    "        currValue [currValue == -1] = 1 #Replace -1 with 1 in case of MIN  \n",
    "        res[row:row+he, col:col+wi] =  np.minimum(currValue, newPredictions)\n",
    "    elif operator == 'MAX':\n",
    "        res[row:row+he, col:col+wi] = np.maximum(currValue, newPredictions)\n",
    "    elif operator == 'PADDING':\n",
    "        newPredictions = newPredictions[padding:he-padding, padding:wi-padding]\n",
    "        res[row+padding:row+he-padding, col+padding:col+wi-padding] = newPredictions\n",
    "    else:\n",
    "        res[row:row+he, col:col+wi] = newPredictions  \n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods that actually makes the predictions\n",
    "def predict_using_model(model, batch, batch_pos, mask,operator,padding):\n",
    "    tm = np.stack(batch, axis = 0)\n",
    "    prediction = model.predict(tm)\n",
    "    for i in range(len(batch_pos)): \n",
    "        (col, row, wi, he) = batch_pos[i]\n",
    "        p = np.squeeze(prediction[i], axis = -1)\n",
    "        # Instead of replacing the current values with new values, use the user specified operator (MIN,MAX,REPLACE)\n",
    "        mask = addTOResult(mask, p, row, col, he, wi,operator,padding)  \n",
    "    return mask\n",
    "    \n",
    "def detect_lake(NDWI_img, width=512, height=512, stride = 256,padding=128):\n",
    "    nols, nrows = NDWI_img.meta['width'], NDWI_img.meta['height']\n",
    "    meta = NDWI_img.meta.copy() \n",
    "    if 'float' not in meta['dtype']: #The prediction is a float so we keep it as float to be consistent with the prediction. \n",
    "        meta['dtype'] = np.float32\n",
    "    col_index=list(range(0, nols-width, stride))\n",
    "    col_index.append(nols-width)\n",
    "    row_index=list(range(0, nrows-height, stride))\n",
    "    row_index.append(nrows-height)\n",
    "    offsets = product(col_index,row_index)\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    print('the size of current NDWI_img',nrows, nols) \n",
    "\n",
    "    mask = np.zeros((nrows, nols), dtype=meta['dtype'])\n",
    "\n",
    "#     mask = mask -1   # Note: The initial mask is initialized with -1 instead of zero   to handle the MIN case (see addToResult)\n",
    "    batch = []\n",
    "    batch_pos = [ ]\n",
    "    for col_off, row_off in  tqdm(offsets):\n",
    "#         if col_off<0:\n",
    "#             col_off=0\n",
    "#             width2=width+col_off\n",
    "#         else:\n",
    "#             width2=width\n",
    "        \n",
    "#         if row_off<0:\n",
    "#             row_off=0\n",
    "#             height2=height+row_off\n",
    "#         else:\n",
    "#             height2=height\n",
    "        window =windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, NDWI_img.transform) \n",
    "#         hbh:-100\n",
    "        patch = np.full((height, width, 1),-1)#Add -1 padding in case of corner images\n",
    "        NDWI_sm = NDWI_img.read(window=window)\n",
    "        temp_im = np.stack(NDWI_sm, axis = -1)\n",
    "        patch[:window.height, :window.width] = temp_im   \n",
    "        batch.append(patch)\n",
    "        batch_pos.append((window.col_off, window.row_off, window.width, window.height))\n",
    "        if (len(batch) == config.BATCH_SIZE):\n",
    "            mask = predict_using_model(model, batch, batch_pos, mask,'PADDING',padding)\n",
    "            batch = []\n",
    "            batch_pos = []\n",
    "            \n",
    "    # To handle the edge of images as the image size may not be divisible by n complete batches and few frames on the edge may be left.\n",
    "    if batch:\n",
    "        mask = predict_using_model(model, batch, batch_pos, mask,'PADDING',padding)\n",
    "        batch = []\n",
    "        batch_pos = []\n",
    "\n",
    "    return(mask, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'geometry': 'Polygon',\n",
    "    'properties': {'id': 'str', 'area': 'float:15.2',},\n",
    "    }\n",
    "def raster2vector(raster_path, vecter_path, field_name=\"class\", ignore_values = None):\n",
    "    \n",
    "    # 读取路径中的栅格数据\n",
    "    raster = gdal.Open(raster_path)\n",
    "    # in_band 为想要转为矢量的波段,一般需要进行转矢量的栅格都是单波段分类结果\n",
    "    # 若栅格为多波段,需要提前转换为单波段\n",
    "    band = raster.GetRasterBand(1)\n",
    "    \n",
    "    # 读取栅格的投影信息,为后面生成的矢量赋予相同的投影信息\n",
    "    prj = osr.SpatialReference()\n",
    "    prj.ImportFromWkt(raster.GetProjection())\n",
    "    \n",
    "    \n",
    "    drv = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "    # 若文件已经存在,删除\n",
    "    if os.path.exists(vecter_path):\n",
    "        drv.DeleteDataSource(vecter_path)\n",
    "        \n",
    "    # 创建目标文件\n",
    "    polygon = drv.CreateDataSource(vecter_path)\n",
    "    # 创建面图层\n",
    "    poly_layer = polygon.CreateLayer(vecter_path[:-4], srs=prj, geom_type=ogr.wkbMultiPolygon)\n",
    "    # 添加浮点型字段,用来存储栅格的像素值\n",
    "    field = ogr.FieldDefn(field_name, ogr.OFTReal)  \n",
    "    poly_layer.CreateField(field)\n",
    "    \n",
    "    # FPolygonize将每个像元转成一个矩形，然后将相似的像元进行合并\n",
    "    # 设置矢量图层中保存像元值的字段序号为0\n",
    "    gdal.FPolygonize(band, None, poly_layer, 0)\n",
    "    \n",
    "    # 删除ignore_value链表中的类别要素\n",
    "    if ignore_values is not None:\n",
    "        for feature in poly_layer:\n",
    "            class_value = feature.GetField('class')\n",
    "            for ignore_value in ignore_values:\n",
    "                if class_value==ignore_value:\n",
    "                    # 通过FID删除要素\n",
    "                    poly_layer.DeleteFeature(feature.GetFID())\n",
    "                    break\n",
    "                \n",
    "    polygon.SyncToDisk()\n",
    "    polygon = None\n",
    "    print('Vector File Exported Successfully!')\n",
    "        \n",
    "def raster_to_polygon(rasterfile,vectorfile,nodata=0):\n",
    "    '''\n",
    "        rasterfile ： 输入要转换的栅格文件\n",
    "    '''\n",
    "    import rasterio as rio\n",
    "    from rasterio import features\n",
    "    from shapely.geometry import shape\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "    \n",
    "    out_shp = gpd.GeoDataFrame(columns=['category','geometry'])\n",
    "\n",
    "    with rio.open(rasterfile) as f:\n",
    "        image = f.read(1)\n",
    "        img_crs = f.crs\n",
    "        image[image == f.nodata] = nodata\n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        i = 0\n",
    "        for coords, value in features.shapes(image, transform=f.transform):\n",
    "            if value != nodata:\n",
    "                geom = shape(coords)\n",
    "                out_shp.loc[i] = [value,geom]\n",
    "                i += 1\n",
    "\n",
    "    out_shp.set_geometry('geometry',inplace=True)\n",
    "    out_shp = out_shp.dissolve(by='category',as_index=False)\n",
    "    out_shp.set_crs(img_crs,inplace=True)\n",
    "    print('raster to polygon have finished!')\n",
    "    out_shp.to_file(vectorfile,encoding='utf-8')\n",
    "    print('Vector File Exported Successfully!')\n",
    "\n",
    "def writeMaskToDisk(detected_mask, detected_meta, wp, write_as_type = 'uint8', th = 0.5, create_countors = False):\n",
    "    # Convert to correct required before writing\n",
    "    if 'float' in str(detected_meta['dtype']) and 'int' in write_as_type:\n",
    "        print(f'Converting prediction from {detected_meta[\"dtype\"]} to {write_as_type}, using threshold of {th}')#float32 to uint8\n",
    "#         initial code have problem of big lake\n",
    "        detected_mask[detected_mask<th]=0\n",
    "        detected_mask[detected_mask>=th]=1\n",
    "        detected_mask = detected_mask.astype(write_as_type)#'uint8'\n",
    "        detected_meta['dtype'] =  write_as_type\n",
    "    \n",
    "    # compress tif\n",
    "    detected_meta.update({\"compress\": 'lzw'})\n",
    "    \n",
    "    with rasterio.open(wp, 'w', **detected_meta) as outds:\n",
    "        outds.write(detected_mask, 1)\n",
    "        \n",
    "#     raster_to_polygon(wp,wp.replace('tif', 'shp'))    \n",
    "#     if create_countors:\n",
    "#         wp = wp.replace('tif', 'shp')\n",
    "#         create_contours_shapefile(detected_mask, detected_meta, wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x21b4bff3800>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.device('/gpu:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142it [00:00, 1419.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('G:\\\\occurrence_img\\\\occurrence_50E60E_40N50N.tif', 'occurrence_50E60E_40N50N.tif')]\n",
      "G:\\occurrence_img\\occurrence_50E60E_40N50N.tif\n",
      "the size of current NDWI_img 40000 40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24336it [07:31, 53.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting prediction from <class 'numpy.float32'> to uint8, using threshold of 0.5\n",
      "Vector File Exported Successfully!\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# Predict trees in the all the files in the input image dir \n",
    "# Depending upon the available RAM, images may not to be split before running this cell.\n",
    "# Use the Auxiliary-2-SplitRasterToAnalyse if the images are too big to be analysed in memory.\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(config.input_image_dir):\n",
    "    for file in files:\n",
    "#         if file.endswith(config.input_image_type) and file.startswith(config.NDWI_fn_st):\n",
    "        if file.endswith(config.input_image_type):\n",
    "             all_files.append((os.path.join(root, file), file))\n",
    "print(all_files)\n",
    "\n",
    "for fullPath, filename in all_files:\n",
    "    outputFile = os.path.join(config.output_dir, filename.replace(config.GSW_fn_st,'padding_random_'+config.output_prefix) )\n",
    "    if not os.path.isfile(outputFile) or config.overwrite_analysed_files: \n",
    "        with rasterio.open(fullPath) as NDWI:\n",
    "            print(fullPath)\n",
    "            detectedMask, detectedMeta = detect_lake(NDWI, width = config.WIDTH, height = config.HEIGHT, stride = config.STRIDE)\n",
    "            # WIDTH and HEIGHT should be the same and in this case Stride is 50 % width\n",
    "            #Write the mask to file\n",
    "            writeMaskToDisk(detectedMask, detectedMeta, outputFile, write_as_type = config.output_dtype, th = 0.5, create_countors = False)            \n",
    "            vecter_path =outputFile.replace('tif','shp')\n",
    "            field_name = \"class\"\n",
    "            # 第0类删除,若实际情况不需要1类和2类,则ignore_values = [1,2]\n",
    "            ignore_values = [0]\n",
    "            raster2vector(outputFile, vecter_path, field_name=field_name, ignore_values = ignore_values)#, ignore_values = ignore_values\n",
    "    else:\n",
    "        print('File already analysed!', fullPath)\n",
    "        \n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster2vector(raster_path, vecter_path, field_name=\"class\", ignore_values = None):\n",
    "    \n",
    "    # 读取路径中的栅格数据\n",
    "    raster = gdal.Open(raster_path)\n",
    "    # in_band 为想要转为矢量的波段,一般需要进行转矢量的栅格都是单波段分类结果\n",
    "    # 若栅格为多波段,需要提前转换为单波段\n",
    "    band = raster.GetRasterBand(1)\n",
    "    \n",
    "    # 读取栅格的投影信息,为后面生成的矢量赋予相同的投影信息\n",
    "    prj = osr.SpatialReference()\n",
    "    prj.ImportFromWkt(raster.GetProjection())\n",
    "    \n",
    "    \n",
    "    drv = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "    # 若文件已经存在,删除\n",
    "    if os.path.exists(vecter_path):\n",
    "        drv.DeleteDataSource(vecter_path)\n",
    "        \n",
    "    # 创建目标文件\n",
    "    polygon = drv.CreateDataSource(vecter_path)\n",
    "    # 创建面图层\n",
    "    poly_layer = polygon.CreateLayer(vecter_path[:-4], srs=prj, geom_type=ogr.wkbMultiPolygon)\n",
    "    # 添加浮点型字段,用来存储栅格的像素值\n",
    "    field = ogr.FieldDefn(field_name, ogr.OFTReal)  \n",
    "    poly_layer.CreateField(field)\n",
    "    \n",
    "    # FPolygonize将每个像元转成一个矩形，然后将相似的像元进行合并\n",
    "    # 设置矢量图层中保存像元值的字段序号为0\n",
    "    gdal.FPolygonize(band, None, poly_layer, 0)\n",
    "    \n",
    "    # 删除ignore_value链表中的类别要素\n",
    "    if ignore_values is not None:\n",
    "        for feature in poly_layer:\n",
    "            class_value = feature.GetField('class')\n",
    "            for ignore_value in ignore_values:\n",
    "                if class_value==ignore_value:\n",
    "                    # 通过FID删除要素\n",
    "                    poly_layer.DeleteFeature(feature.GetFID())\n",
    "                    break\n",
    "                \n",
    "    polygon.SyncToDisk()\n",
    "    polygon = None\n",
    "    print('Vector File Exported Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for root, dirs, files in os.walk(r'H:\\毕业设计\\GISA\\China'):\n",
    "    for file in files:\n",
    "#         if file.endswith(config.input_image_type) and file.startswith(config.NDWI_fn_st):\n",
    "        if file.endswith('tif'):\n",
    "             all_files.append((os.path.join(root, file), file))\n",
    "print(all_files)\n",
    "\n",
    "for fullPath, filename in all_files:\n",
    "    outputFile = fullPath.replace('tif', '_2shp_dn1.shp') \n",
    "    field_name = \"class\"\n",
    "    # 第0类删除,若实际情况不需要1类和2类,则ignore_values = [1,2]\n",
    "    ignore_values = [0]\n",
    "    raster2vector(fullPath, outputFile, field_name=field_name, ignore_values = ignore_values)#, ignore_values = ignore_values\n",
    "        \n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

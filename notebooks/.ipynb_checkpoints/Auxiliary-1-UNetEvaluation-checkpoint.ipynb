{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Author: Ankit Kariryaa, University of Bremen\n",
    "  \n",
    "  Modified by Xuehui Pi and Qiuqi Luo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np               # numerical array manipulation\n",
    "import pandas as pd\n",
    "import geopandas as gps\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from PIL import Image\n",
    "import rasterio                  # I/O raster data (netcdf, height, geotiff, ...)\n",
    "import rasterio.warp             # Reproject raster samples\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.geometry import mapping, shape\n",
    "import fiona\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import rasterio.mask\n",
    "import affine\n",
    "\n",
    "from core.UNet import UNet\n",
    "from core.losses import tversky, accuracy, dice_coef, dice_loss, specificity, sensitivity,mIoU\n",
    "from core.optimizers import adaDelta, adagrad, adam, nadam\n",
    "from core.frame_info_evaluate import FrameInfo\n",
    "from core.dataset_generator import DataGenerator\n",
    "from core.split_frames import split_dataset1,split_dataset2,split_dataset3,split_dataset4#,split_dataset5\n",
    "from core.visualize import display_images\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # plotting tools\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import warnings                  # ignore annoying warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto(\n",
    "    #device_count={\"CPU\": 64},\n",
    "    allow_soft_placement=True, \n",
    "    log_device_placement=False)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data related variables used in the notebook \n",
    "\n",
    "# For reading the GSW and annotated images generated in the step - 1\n",
    "\n",
    "base_dir = '' \n",
    "auxiliary_data_dir = ''\n",
    "\n",
    "# The Normal model was consisted of type 1, 2, 3, and 4, while the Floodplain Model was made up of types 1, 3, 4, and 5.\n",
    "path_to_write1 = os.path.join(base_dir,'output\\output1')\n",
    "path_to_write2 = os.path.join(base_dir,'output\\output2')\n",
    "path_to_write3 = os.path.join(base_dir,'output\\output3')\n",
    "path_to_write4 = os.path.join(base_dir,'output\\output4')\n",
    "# path_to_write5 = os.path.join(base_dir,'output\\output5')\n",
    "\n",
    "image_type = '.png'\n",
    "GSW_fn = 'occurrence'\n",
    "annotation_fn = 'annotation'\n",
    "\n",
    "# For testing, images are divided into sequential patches \n",
    "patch_generation_stratergy = 'sequential'\n",
    "patch_size = (512,512,2) ## Height * Width * (Input or Output) channelsï¼š[GSW, ANNOTATION]\n",
    "BATCH_SIZE = 16 # Model is evaluated in batches; See https://keras.io/models/model/\n",
    "\n",
    "# # When stratergy == sequential\n",
    "step_size = (512,512)\n",
    "\n",
    "\n",
    "patch_dir = os.path.join(base_dir, 'patches{}'.format(patch_size[0])) \n",
    "frames_json1 = os.path.join(patch_dir,'frames_list1.json') \n",
    "frames_json2 = os.path.join(patch_dir,'frames_list2.json')\n",
    "frames_json3 = os.path.join(patch_dir,'frames_list3.json')\n",
    "frames_json4 = os.path.join(patch_dir,'frames_list4.json')\n",
    "# frames_json5 = os.path.join(patch_dir,'frames_list5.json')\n",
    "\n",
    "input_shape = (512,512,1)\n",
    "input_image_channel = [0]\n",
    "input_label_channel = [1]\n",
    "\n",
    "OPTIMIZER = adaDelta \n",
    "OPTIMIZER=tf.train.experimental.enable_mixed_precision_graph_rewrite(OPTIMIZER)\n",
    "LOSS = tversky\n",
    "\n",
    "#Only for the name of the model in the very end\n",
    "OPTIMIZER_NAME = 'adaDelta'\n",
    "LOSS_NAME = 'tversky'\n",
    "modelToEvaluate = os.path.join(base_dir, '') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File path for final report \n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "chf = input_image_channel + input_label_channel\n",
    "chs = reduce(lambda a,b: a+str(b),   chf, '')\n",
    "evaluation_report_path = model_path =  os.path.join(base_dir, 'evaluationreport') \n",
    "if not os.path.exists(evaluation_report_path):\n",
    "    os.makedirs(evaluation_report_path)\n",
    "evaluation_report_filename = os.path.join(evaluation_report_path,'evaluation_per_pixel{}_{}.csv'.format(timestr,chs))\n",
    "print(evaluation_report_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read all images/frames into memory  \n",
    "frames1 = []\n",
    "\n",
    "all_files = os.listdir(path_to_write1)\n",
    "all_files_GSW = [fn for fn in all_files if fn.startswith(GSW_fn) and fn.endswith(image_type)]\n",
    "len(all_files_GSW)\n",
    "print(all_files_GSW)\n",
    "for i, fn in enumerate(all_files_GSW):\n",
    "    GSW_img = rasterio.open(os.path.join(path_to_write1, fn))\n",
    "    read_GSW_img = GSW_img.read()\n",
    "    GSW_img_meta = GSW_img.meta\n",
    "    comb_img = np.transpose(read_GSW_img, axes=(1,2,0)) \n",
    "    \n",
    "    annotation_im = Image.open(os.path.join(path_to_write1, fn.replace(GSW_fn,annotation_fn)))\n",
    "    annotation = np.array(annotation_im)\n",
    "    f = FrameInfo(comb_img, annotation,GSW_img_meta)\n",
    "    frames1.append(f)\n",
    "print(len(frames1))\n",
    "    \n",
    "training_frames1, validation_frames1, testing_frames1  = split_dataset1(frames1, frames_json1, patch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames2 = []\n",
    "\n",
    "all_files = os.listdir(path_to_write2)\n",
    "all_files_GSW = [fn for fn in all_files if fn.startswith(GSW_fn) and fn.endswith(image_type)]\n",
    "len(all_files_GSW)\n",
    "print(all_files_GSW)\n",
    "for i, fn in enumerate(all_files_GSW):\n",
    "    GSW_img = rasterio.open(os.path.join(path_to_write2, fn))\n",
    "    read_GSW_img = GSW_img.read()\n",
    "    GSW_img_meta = GSW_img.meta\n",
    "    comb_img = np.transpose(read_GSW_img, axes=(1,2,0))\n",
    "    \n",
    "    annotation_im = Image.open(os.path.join(path_to_write2, fn.replace(GSW_fn,annotation_fn)))\n",
    "    annotation = np.array(annotation_im)\n",
    "    f = FrameInfo(comb_img, annotation,GSW_img_meta)\n",
    "    frames2.append(f)\n",
    "\n",
    "frames_12=frames1+frames2\n",
    "print(len(frames_12))\n",
    "\n",
    "training_frames2, validation_frames2, testing_frames2  = split_dataset2(frames1,frames_12,frames_json2, patch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames3 = []\n",
    "\n",
    "all_files = os.listdir(path_to_write3)\n",
    "all_files_GSW = [fn for fn in all_files if fn.startswith(GSW_fn) and fn.endswith(image_type)]\n",
    "len(all_files_GSW)\n",
    "print(all_files_GSW)\n",
    "for i, fn in enumerate(all_files_GSW):\n",
    "    GSW_img = rasterio.open(os.path.join(path_to_write3, fn))\n",
    "    read_GSW_img = GSW_img.read()\n",
    "    GSW_img_meta = GSW_img.meta\n",
    "    comb_img = np.transpose(read_GSW_img, axes=(1,2,0))\n",
    "    \n",
    "    annotation_im = Image.open(os.path.join(path_to_write3, fn.replace(GSW_fn,annotation_fn)))\n",
    "    annotation = np.array(annotation_im)\n",
    "    f = FrameInfo(comb_img, annotation,GSW_img_meta)\n",
    "    frames3.append(f)\n",
    "\n",
    "frames_123=frames1+frames2+frames3\n",
    "print(len(frames_123))\n",
    "\n",
    "training_frames3, validation_frames3, testing_frames3  = split_dataset3(frames_12,frames_123, frames_json3, patch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames4 = []\n",
    "\n",
    "all_files = os.listdir(path_to_write4)\n",
    "all_files_GSW = [fn for fn in all_files if fn.startswith(GSW_fn) and fn.endswith(image_type)]\n",
    "len(all_files_GSW)\n",
    "print(all_files_GSW)\n",
    "for i, fn in enumerate(all_files_GSW):\n",
    "    GSW_img = rasterio.open(os.path.join(path_to_write4, fn))\n",
    "    read_GSW_img = GSW_img.read()\n",
    "    GSW_img_meta = GSW_img.meta\n",
    "    comb_img = np.transpose(read_GSW_img, axes=(1,2,0)) \n",
    "    \n",
    "    annotation_im = Image.open(os.path.join(path_to_write4, fn.replace(GSW_fn,annotation_fn)))\n",
    "    annotation = np.array(annotation_im)\n",
    "    f = FrameInfo(comb_img, annotation,GSW_img_meta)\n",
    "    frames4.append(f)\n",
    "    \n",
    "frames_1234=frames1+frames2+frames3+frames4\n",
    "print(len(frames_1234))\n",
    "\n",
    "training_frames4, validation_frames4, testing_frames4  = split_dataset4(frames_123,frames_1234, frames_json4,patch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing on all frames. All sequential frames are kept in memory and this may create memory related errors in some cases. \n",
    "\n",
    "testing_frames=testing_frames1+testing_frames2+testing_frames3+testing_frames4\n",
    "annotation_channels = input_label_channel\n",
    "\n",
    "test_generator = DataGenerator(input_image_channel, patch_size, testing_frames, frames_1234, annotation_channels)\n",
    "# Sequential generate all patches from the all frames \n",
    "test_patches = test_generator.all_sequential_patches(step_size)#step_size = (512ï¼Œ512)   \n",
    "print('Total patches to evaluate the model on: ' + str(len(test_patches[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the some of the test images\n",
    "numberOfImagesToDisplay = 5\n",
    "\n",
    "train_images, real_label = test_patches[0][:numberOfImagesToDisplay], test_patches[1][:numberOfImagesToDisplay]\n",
    "display_images(np.concatenate((train_images,real_label), axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model \n",
    "\n",
    "def evaluate_model(model_path, evaluation_report_filename):\n",
    "    print(model_path, evaluation_report_filename)\n",
    "    model = load_model(model_path, custom_objects={'tversky': tversky, 'dice_coef': dice_coef, 'dice_loss':dice_loss, 'accuracy':accuracy ,'mIoU':mIoU, 'specificity': specificity, 'sensitivity':sensitivity}, compile=False)\n",
    "\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[dice_coef, dice_loss, accuracy,mIoU, specificity, sensitivity])\n",
    "    \n",
    "    print('Evaluating model now!')\n",
    "    ev = model.evaluate(x=test_patches[0], y=test_patches[1],  verbose=1, use_multiprocessing=False)\n",
    "    report  = dict(zip(model.metrics_names, ev))\n",
    "    report['model_path'] =  model_path   \n",
    "    report['test_frame_dir']= base_dir   \n",
    "    report['total_patch_count']= len(test_patches[0])  \n",
    "    return report\n",
    "\n",
    "report = evaluate_model(modelToEvaluate, evaluation_report_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the model predictions! \n",
    "\n",
    "model = load_model(modelToEvaluate, custom_objects={'tversky': tversky, 'dice_coef': dice_coef, 'dice_loss':dice_loss, 'accuracy':accuracy ,'mIoU':mIoU, 'specificity': specificity, 'sensitivity':sensitivity}, compile=False)\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[dice_coef, dice_loss, accuracy,mIoU, specificity, sensitivity])\n",
    "\n",
    "grwl_path = os.path.join(auxiliary_data_dir, 'GRWL') \n",
    "OSM_path =  os.path.join(auxiliary_data_dir, 'OSM_water') \n",
    "occurrence_eq0_path = os.path.join(auxiliary_data_dir, 'occurrence_eq0_sum.gdb')\n",
    "outpatches = os.path.join(base_dir, 'predictions_patches')\n",
    "if not os.path.exists(outpatches):\n",
    "    os.makedirs(outpatches)\n",
    "\n",
    "# load patch meta tempelate for further edition\n",
    "with rasterio.open(os.path.join(auxiliary_data_dir, 'occurrence/occurrence_0E10E_0N10N.tif')) as raster_template:\n",
    "    #print(raster_template.meta)\n",
    "    meta_template = raster_template.meta \n",
    "\n",
    "predictions, predictions_masked = [], []\n",
    "for i, tp in enumerate(test_patches[0]):\n",
    "    tpx = np.expand_dims(tp, axis=0)\n",
    "    modelpredtictions = model.predict(tpx, batch_size=BATCH_SIZE) # 1*512*512*1\n",
    "    modelpredtictions = np.squeeze(modelpredtictions, axis = 3) # 1*512*512\n",
    "#     print(modelpredtictions.shape)\n",
    "    \n",
    "    # save ndarray of pathces to tif\n",
    "    patch_lonlat_ul = test_patches[2][i]\n",
    "    patch_lon_left = patch_lonlat_ul[0]\n",
    "    patch_lat_up = patch_lonlat_ul[1]\n",
    "    patch_transform = affine.Affine(0.00025, 0.0, patch_lon_left, 0.0, -0.00025, patch_lat_up)\n",
    "    patch_meta = meta_template.copy()\n",
    "    patch_meta.update({\n",
    "              \"dtype\": 'float32',\n",
    "              \"height\": patch_size[0],\n",
    "              \"width\": patch_size[1],\n",
    "              \"transform\": patch_transform,\n",
    "              \"compress\": 'lzw'}) #\n",
    "#     print('patch_meta', patch_meta)\n",
    "    with rasterio.open(outpatches+'/patches_'+str(i)+'.tif', 'w', **patch_meta) as prediction_patch_dataset:\n",
    "        prediction_patch_dataset.write(modelpredtictions)\n",
    "    \n",
    "    # select all types of masks based on the pathces bbox\n",
    "    bbox = rasterio.transform.array_bounds(patch_meta['height'], patch_meta['width'], patch_transform)\n",
    "    grwl_dn255_patch = gps.read_file(grwl_path+'/GRWL_DN255.shp', bbox=bbox) # data from GRWL_mask_V01.01 product with DN = 255 (River) (see https://doi.org/10.5281/zenodo.1297434).\n",
    "    grwl_dn126_patch = gps.read_file(grwl_path+'/GRWL_DN126.shp', bbox=bbox) # data from GRWL_mask_V01.01 product with DN = 126 (Tidal rivers/delta) (see https://doi.org/10.5281/zenodo.1297434).\n",
    "    grwl_dn86_patch = gps.read_file(grwl_path+'/GRWL_DN86.shp', bbox=bbox) # data from GRWL_mask_V01.01 product with DN = 126 (Canal) (see https://doi.org/10.5281/zenodo.1297434).\n",
    "    OSM_patch = gps.read_file(OSM_path+'/mergeocean.shp', bbox=bbox) # data from OSMWL with gridcode = 1 (Ocean) (see http://hydro.iis.u-tokyo.ac.jp/~yamadai/OSM_water/index.html).\n",
    "    occurrence_eq0_patch = gps.read_file(occurrence_eq0_path, layer='occurrence_eq0', bbox=bbox) # data from GSWO with value = 0 (Land / Not water), available at https://global-surface-water.appspot.com.\n",
    "    mask_patch = grwl_dn255_patch.append(grwl_dn126_patch).append(grwl_dn86_patch).append(OSM_patch).append(occurrence_eq0_patch)\n",
    "    mask_patch_geojson = mask_patch.geometry.values \n",
    "    \n",
    "    # perform river mask and ocean mask\n",
    "    with rasterio.open(outpatches+'/patches_'+str(i)+'.tif', 'r') as prediction_patch_dataset:\n",
    "        if len(mask_patch_geojson) > 0:\n",
    "            modelpredtictions_masked, out_meta = rasterio.mask.mask(prediction_patch_dataset, mask_patch_geojson, all_touched=True, invert=True)\n",
    "            print(f'Mask finished: {i+1} / {len(test_patches[0])}')\n",
    "        else:\n",
    "            modelpredtictions_masked = modelpredtictions.copy()\n",
    "            print(f'No masks detected, output original image instead: {i+1} / {len(test_patches[0])}')\n",
    "    with rasterio.open(outpatches+'/patches_masked_'+str(i)+'.tif', \"w\", **patch_meta) as prediction_masked_patch_dataset:\n",
    "        prediction_masked_patch_dataset.write(modelpredtictions_masked)\n",
    "\n",
    "    predictions.append(np.transpose(modelpredtictions, axes=(1,2,0)))\n",
    "    predictions_masked.append(np.transpose(modelpredtictions_masked, axes=(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Contours from image to world coordinates  \n",
    "def transform_contours_to_xy(contours, transform):\n",
    "    tp = []\n",
    "    for cnt in contours:\n",
    "        pl = cnt[:, 0, :]\n",
    "        cols, rows = zip(*pl)\n",
    "        x,y = rasterio.transform.xy(transform, rows, cols)\n",
    "        if not isinstance(x, list):\n",
    "            x = [x]\n",
    "            y = [y]\n",
    "        tl = [list(i) for i in zip(x, y)]\n",
    "        tp.append(tl)\n",
    "    return (tp)\n",
    "\n",
    "def mask_to_polygons(mask, transform,j,th = 0.5):\n",
    "    # first, find contours with cv2: it's much faster than shapely and returns hierarchy \n",
    "    mask[mask < th] = 0 \n",
    "    mask[mask >= th] = 1\n",
    "    mask = ((mask) * 255).astype(np.uint8)\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    \n",
    "    #Convert contours from image coordinate to xy coordinate (world coordinates) \n",
    "    contours = transform_contours_to_xy(contours, transform)\n",
    "    \n",
    "    if contours:\n",
    "        print(f'Finish contours/polygons detected in: {j+1} / {len(test_patches[0])}')\n",
    "    else: #TODO: Raise an error maybe\n",
    "        print(f'No detected in: {j+1} / {len(test_patches[0])}')\n",
    "        return [Polygon()]\n",
    "    \n",
    "    # now messy stuff to associate parent and child contours \n",
    "    cnt_children = defaultdict(list)\n",
    "    child_contours = set()\n",
    "    assert hierarchy.shape[0] == 1\n",
    "    # http://docs.opencv.org/3.1.0/d9/d8b/tutorial_py_contours_hierarchy.html\n",
    "    for idx, (_, _, _, parent_idx) in enumerate(hierarchy[0]):\n",
    "        if parent_idx != -1:\n",
    "            child_contours.add(idx)\n",
    "            cnt_children[parent_idx].append(contours[idx])\n",
    "            '''for (key, value) in data:   result[key].append(value)'''\n",
    "            \n",
    "    # create actual polygons filtering by area/hole (removes artifacts)  \n",
    "    all_polygons = []\n",
    "#     min_area=100 #gt 0.03km2\n",
    "    for idx, cnt in enumerate(contours):\n",
    "        if idx not in child_contours: #and cv2.contourArea(cnt) >= min_area: #Do we need to check for min_area??\n",
    "            try:\n",
    "                poly = Polygon(\n",
    "                    shell=cnt,\n",
    "                    holes=[c for c in cnt_children.get(idx, []) ])\n",
    "                           #if cv2.contourArea(c) >= min_area]) #Do we need to check for min_area??\n",
    "#                     holes=[c for c in cnt_children.get(idx, []) if cv2.contourArea(c) >= min_area])\n",
    "                all_polygons.append(poly)\n",
    "            except Exception as e: \n",
    "#                 print(e)\n",
    "                pass   \n",
    "    #print(len(all_polygons))\n",
    "    return(all_polygons)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj    \n",
    "import shapely\n",
    "import shapely.ops as ops\n",
    "\n",
    "def transform_to_meter_coordinate_system(geom):\n",
    "    # TODO: Remove the hard coded values\n",
    "    project = pyproj.Transformer.from_proj(\n",
    "        pyproj.Proj(init='epsg:4326'), # source coordinate system  \n",
    "        pyproj.Proj(init='epsg:8857')) # destination coordinate system   \n",
    "    gt = ops.transform(project.transform, geom)  # apply projection\n",
    "    return gt\n",
    "\n",
    "def ha_area(ha_polygons):\n",
    "    ts = 0\n",
    "    ha_polygons_meter = [transform_to_meter_coordinate_system(p) for p in ha_polygons]\n",
    "    for p in ha_polygons_meter:\n",
    "        if p.area > 0.03*(10**6):#minarea=0.015*10^6\n",
    "            ts += p.area\n",
    "    return ts\n",
    "\n",
    "def ha_area_polygons(ha_polygons):\n",
    "    polygons_number=0\n",
    "    ha_polygons_meter = [transform_to_meter_coordinate_system(p) for p in ha_polygons]\n",
    "    for p in ha_polygons_meter:\n",
    "        if p.area > 0.03*(10**6):#minarea=0.015*10^6\n",
    "            polygons_number =polygons_number+1\n",
    "#     area_polygonsnumber=np.concatenate([ts,polygons_number],axis=1)\n",
    "    return polygons_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Remove the need for the a tiff file for transformation \n",
    "ha_prediction_polygons = []\n",
    "ha_label_polygons = []\n",
    "    \n",
    "ha_prediction_canopy_area = []\n",
    "ha_label_canopy_area = []\n",
    "\n",
    "for em, pred in enumerate(predictions_masked):\n",
    "    with rasterio.open(outpatches+'/patches_masked_'+str(em)+'.tif', \"r\") as raster_image:\n",
    "#         print(raster_image.meta)\n",
    "        transform = raster_image.meta['transform']\n",
    "#         print(transform)\n",
    "    ap = mask_to_polygons(pred, transform,em)\n",
    "#     print(ap)\n",
    "#     createShapefileObject(ap, raster_image.meta, outpatches,em)\n",
    "    ha_prediction_polygons.append(ap)\n",
    "#     ha_prediction_canopy_area.append(ha_area_from_mask(pred))\n",
    "    \n",
    "for i, lb in enumerate(test_patches[1][...,[0]]):\n",
    "    with rasterio.open(outpatches+'/patches_masked_'+str(i)+'.tif', \"r\") as raster_image:\n",
    "        #print(raster_image.meta)\n",
    "        transform = raster_image.meta['transform']\n",
    "            \n",
    "    ap = mask_to_polygons(lb, transform,i)#\n",
    "    ha_label_polygons.append(ap)\n",
    "# ha_label_canopy_area.append(ha_area_from_mask(lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha_prediction_polygons_count = [len(hp) for hp in ha_prediction_polygons] \n",
    "ha_label_polygons_count = [len(hp) for hp in ha_label_polygons] \n",
    "\n",
    "# Alternate method to calcualte canopy area as a sum of area of trees  \n",
    "print('To calculate the area we convert the polygons to a coordinate system where unit is meters. This process is time consuming.')\n",
    "\n",
    "ha_prediction_canopy_area= list(map(ha_area, ha_prediction_polygons))\n",
    "ha_prediction_polygons_number=list(map(ha_area_polygons, ha_prediction_polygons))\n",
    "\n",
    "ha_label_canopy_area= list(map(ha_area, ha_label_polygons))\n",
    "ha_label_polygons_number=list(map(ha_area_polygons, ha_label_polygons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "print(len(ha_prediction_polygons_number))\n",
    "ha_count_correlation = pearsonr(ha_prediction_polygons_number, ha_label_polygons_number)\n",
    "ha_area_correlation = pearsonr(ha_prediction_canopy_area, ha_label_canopy_area)\n",
    "\n",
    "print('Count correlation:' + str(ha_count_correlation))\n",
    "print('Area correlation:' + str(ha_area_correlation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the final report\n",
    "report['count_correlation'] =  ha_count_correlation[0]\n",
    "report['count_correlation_tvalue'] =  ha_count_correlation[1]\n",
    "report['area_correlation'] = ha_area_correlation[0]\n",
    "report['area_correlation_tvalue'] =  ha_area_correlation[1]\n",
    "\n",
    "print(report)\n",
    "\n",
    "tdf = pd.DataFrame(report, index=[0])  \n",
    "print(tdf.columns)\n",
    "col_beginning = ['model_path','test_frame_dir', 'total_patch_count', 'accuracy', 'sensitivity']\n",
    "\n",
    "col_rest = [x for x in tdf.columns.tolist() if x not in col_beginning]\n",
    "cols = col_beginning + col_rest\n",
    "tdf = tdf[cols]\n",
    "tdf.to_csv(evaluation_report_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha_label_canopy_area_km2 = [x/1e6 for x in ha_label_canopy_area]\n",
    "ha_prediction_canopy_area_km2 = [x/1e6 for x in ha_prediction_canopy_area]\n",
    "# print(ha_label_canopy_area_km2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5),dpi=150) \n",
    "\n",
    "ax.plot((0, 1), (0, 1), transform=ax.transAxes, ls='--',c='k')\n",
    "\n",
    "\n",
    "plt.scatter(ha_label_polygons_number,ha_prediction_polygons_number,s=20,c='b',alpha=.4, marker='o')\n",
    "\n",
    "plt.tick_params(labelsize=10)\n",
    "labels = ax.get_xticklabels() + ax.get_yticklabels()\n",
    "[label.set_fontname('Times New Roman') for label in labels]\n",
    "\n",
    "\n",
    "res = stats.linregress(ha_label_polygons_number, ha_prediction_polygons_number) \n",
    "plt.plot(ha_label_polygons_number, res.intercept + res.slope*np.array(ha_label_polygons_number), 'r')\n",
    "RMSE = mean_squared_error(ha_label_polygons_number, ha_prediction_polygons_number, squared=False)#RMSE\n",
    "\n",
    "ax.text(5,75,f\"Slope: {res.slope:.3f}\\nR-squared: {res.rvalue**2:.3f}\\nRMSE: {RMSE:.3f}\\nPatch:{len(ha_label_canopy_area_km2)}\",fontproperties='Times New Roman')\n",
    "plt.xlabel('Label density',fontproperties='Times New Roman')    \n",
    "plt.ylabel('Prediction density',fontproperties='Times New Roman')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_polygonsnumber = pd.DataFrame({\n",
    "    'ha_label_polygons_number': ha_label_polygons_number,\n",
    "    'ha_prediction_polygons_number': ha_prediction_polygons_number,\n",
    "})\n",
    "patch_polygonsnumber.to_csv(os.path.join(evaluation_report_path, 'a_patch_polygonsnumber.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5),dpi=150) \n",
    "ax.plot((0, 1), (0, 1), transform=ax.transAxes, ls='--',c='k')\n",
    "\n",
    "plt.scatter(ha_label_canopy_area_km2,ha_prediction_canopy_area_km2,s=20,c='b',alpha=.4, marker='o')\n",
    "\n",
    "plt.tick_params(labelsize=10)\n",
    "labels = ax.get_xticklabels() + ax.get_yticklabels()\n",
    "[label.set_fontname('Times New Roman') for label in labels]\n",
    "\n",
    "\n",
    "res = stats.linregress(ha_label_canopy_area_km2, ha_prediction_canopy_area_km2) \n",
    "\n",
    "plt.plot(ha_label_canopy_area_km2, res.intercept + res.slope*np.array(ha_label_canopy_area_km2), 'r')\n",
    "RMSE = mean_squared_error(ha_label_canopy_area_km2, ha_prediction_canopy_area_km2, squared=False)#RMSE\n",
    "\n",
    "ax.text(5,60,f\"Slope: {res.slope:.3f}\\nR-squared: {res.rvalue**2:.3f}\\nRMSE: {RMSE:.3f}\\nPatch:{len(ha_label_canopy_area_km2)}\",fontproperties='Times New Roman')\n",
    "plt.xlabel('Label area (${km^2}$)',fontproperties='Times New Roman')    \n",
    "plt.ylabel('Prediction area (${km^2}$)',fontproperties='Times New Roman')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_area_df = pd.DataFrame({\n",
    "    'ha_label_canopy_area_km2': ha_label_canopy_area_km2,\n",
    "    'ha_prediction_canopy_area_km2': ha_prediction_canopy_area_km2,\n",
    "})\n",
    "patch_area_df.to_csv(os.path.join(evaluation_report_path, 'a_patch_area_km2.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
